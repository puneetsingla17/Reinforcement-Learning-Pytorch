{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Crossentropy.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikkhJbnZhtno",
        "colab_type": "code",
        "outputId": "efb275d8-c8b2-45b2-d955-5048d070c4ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 644
        }
      },
      "source": [
        "!pip install gym\n",
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install piglet\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.15.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.12.0)\n",
            "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from gym) (4.1.2.30)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.3)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.17.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym) (0.16.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python-opengl is already the newest version (3.1.0+dfsg-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.3).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.6/dist-packages (0.2.5)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.6/dist-packages (from pyvirtualdisplay) (0.2.8)\n",
            "Requirement already satisfied: piglet in /usr/local/lib/python3.6/dist-packages (0.4.4)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.6/dist-packages (from piglet) (1.1.1)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.6/dist-packages (from piglet) (1.6.3)\n",
            "Requirement already satisfied: Parsley in /usr/local/lib/python3.6/dist-packages (from piglet) (1.3)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from piglet) (19.3.0)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet) (1.12.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet) (0.33.6)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "xdpyinfo was not found, X start can not be checked! Please install xdpyinfo!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1005'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1005'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wI4bq7yFitUc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# This code creates a virtual display to draw game images on. \n",
        "# If you are running locally, just ignore it\n",
        "import os\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
        "    !bash ../xvfb start\n",
        "    %env DISPLAY=:1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkwKpia8ivcw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sScrLwlcixkQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make(\"CartPole-v0\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcaPq9GRj7Tq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as n\n",
        "import torch.optim as op\n",
        "import torch.nn.functional as f"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbYfiCKDlSiK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "observationsize=env.observation_space.shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8Ry0PATmDPb",
        "colab_type": "code",
        "outputId": "fbec00d8-e660-4aca-8752-d3cb323da105",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "observationsize"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4piwYd3mFV_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hiddensize=128\n",
        "batchsize=16\n",
        "percent=70\n",
        "from collections import namedtuple"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsV95SmYmTI4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class net(n.Module):\n",
        "  def __init__(self,obs_size,actionsize,nactions):\n",
        "    super().__init__()\n",
        "    self.fc1=n.Linear(obs_size,hiddensize)\n",
        "    self.fc2=n.Linear(hiddensize,actionsize)\n",
        "  \n",
        "  def forward(self,x):\n",
        "    out=f.relu(self.fc1(x))\n",
        "    out=self.fc2(out)\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYHjf9JGrW2n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import namedtuple"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skNZzJ5BrdCv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "episode1=namedtuple(\"episode\",field_names=['reward','steps'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qg1x8oMyrnKz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "e1=episode1(1,12)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFK1yWhKrxBz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "episodestep=namedtuple(\"episodestep\",field_names=['observation','action'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iy5gI6RrxqD",
        "colab_type": "code",
        "outputId": "24d880b8-1dc7-4a63-e044-1503683b2c55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "e1.reward"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dF3t3FilsKAT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as n\n",
        "import torch.optim as op\n",
        "import torch.nn.functional as f"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mloy2jtOsLSz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def iterate_batches(env,net,batchsize):\n",
        "  batch=[]\n",
        "  episodereward=0\n",
        "  episodesteps=[]\n",
        "  obs=env.reset()\n",
        "  sm=n.Softmax(dim=1)\n",
        "  while True:\n",
        "    obs_v=torch.FloatTensor([obs])\n",
        "    acs_probs_v=net(obs_v)\n",
        "    acs_probs=sm(acs_probs_v)\n",
        "    acs_probs=acs_probs.data.numpy()[0]\n",
        "    act=np.random.choice(len(acs_probs),p=acs_probs)\n",
        "    #print(action)\n",
        "    nextobs,reward,isdone,_=env.step(act)\n",
        "    episodereward+=reward\n",
        "    episodesteps.append(episodestep(observation=obs,action=act))\n",
        "    if isdone:\n",
        "      batch.append(episode1(reward=episodereward,steps=episodesteps))\n",
        "      episodereward=0\n",
        "      episodesteps=[]\n",
        "      nextobs=env.reset()\n",
        "      if len(batch)==batchsize:\n",
        "        yield batch\n",
        "        batch=[]\n",
        "    obs=nextobs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S76mwwhCSa6n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def filter_batch(batch,percentile):\n",
        "  rewards=list(map(lambda s:s.reward,batch))\n",
        "  rewards_bound=np.percentile(rewards,percentile)\n",
        "  rewardsmean=float(np.mean(rewards))\n",
        "  train_obs=[]\n",
        "  train_act=[]\n",
        "  #print(batch[0])\n",
        "  for example in batch:\n",
        "    if example.reward<rewards_bound:\n",
        "      continue\n",
        "    #print(example.steps)\n",
        "    train_obs.extend(map(lambda x:x.observation,example.steps))\n",
        "    train_act.extend(map(lambda x:x.action,example.steps))\n",
        "  \n",
        "  train_obs_v=torch.FloatTensor(train_obs)\n",
        "  train_act_v=torch.LongTensor(train_act)\n",
        "  return train_obs_v,train_act_v,rewards_bound,rewardsmean\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3f3wjW-btfJ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env=gym.make(\"CartPole-v0\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grain5aCt-Dy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "obssize=env.observation_space.shape[0]\n",
        "n_actions=env.action_space.n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQ2LWX2Aueaq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net1=net(obssize,2,4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-SvMRA0vVmS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "01c191c3-1c52-4579-9692-a1a6501ba450"
      },
      "source": [
        "env.action_space.n"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTsBdeDzvaOS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "399e2e27-ebd9-4b1e-d3f1-a016f5cb0318"
      },
      "source": [
        "sum(x.numel() for x in net1.parameters())"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "898"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RmARyhivjNS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "objective=n.CrossEntropyLoss()\n",
        "optimizer=op.Adam(net1.parameters(),lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYmLFo13wpt6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9e57572a-27e3-44c0-f83a-29a432b0ca58"
      },
      "source": [
        "count=0\n",
        "for iter_no,batch in enumerate(iterate_batches(env,net1,batchsize)):\n",
        "  obs_v,acts_v,rewards_b,reward_m=filter_batch(batch,70)\n",
        "  optimizer.zero_grad()\n",
        "  action_scores_v=net1(obs_v)\n",
        "  loss_v=objective(action_scores_v,acts_v)\n",
        "  loss_v.backward()\n",
        "  optimizer.step()\n",
        "  print(\"loss:-\",loss_v.item(),\" reward mean:-\",reward_m,\" reward bound:-\",rewards_b)\n",
        "  count+=1\n",
        "  if count==500:\n",
        "    break"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss:- 0.6843963861465454  reward mean:- 22.9375  reward bound:- 24.0\n",
            "loss:- 0.6894940137863159  reward mean:- 21.5  reward bound:- 22.5\n",
            "loss:- 0.6976339817047119  reward mean:- 20.0625  reward bound:- 23.5\n",
            "loss:- 0.6888865232467651  reward mean:- 22.0  reward bound:- 21.0\n",
            "loss:- 0.691582202911377  reward mean:- 21.6875  reward bound:- 26.0\n",
            "loss:- 0.6839491128921509  reward mean:- 25.0  reward bound:- 31.5\n",
            "loss:- 0.6856576800346375  reward mean:- 21.6875  reward bound:- 27.5\n",
            "loss:- 0.6802443861961365  reward mean:- 27.8125  reward bound:- 30.5\n",
            "loss:- 0.6818201541900635  reward mean:- 25.125  reward bound:- 30.0\n",
            "loss:- 0.6816416382789612  reward mean:- 24.6875  reward bound:- 26.5\n",
            "loss:- 0.6811050176620483  reward mean:- 34.0  reward bound:- 37.5\n",
            "loss:- 0.6817665696144104  reward mean:- 25.1875  reward bound:- 31.5\n",
            "loss:- 0.6733493804931641  reward mean:- 30.0  reward bound:- 36.5\n",
            "loss:- 0.6766272187232971  reward mean:- 29.5625  reward bound:- 37.5\n",
            "loss:- 0.6772396564483643  reward mean:- 28.1875  reward bound:- 30.0\n",
            "loss:- 0.6741304397583008  reward mean:- 32.0  reward bound:- 37.5\n",
            "loss:- 0.6755644679069519  reward mean:- 31.0625  reward bound:- 37.0\n",
            "loss:- 0.6742581725120544  reward mean:- 31.1875  reward bound:- 38.0\n",
            "loss:- 0.6673875451087952  reward mean:- 28.375  reward bound:- 30.5\n",
            "loss:- 0.6684724688529968  reward mean:- 33.375  reward bound:- 36.5\n",
            "loss:- 0.6675098538398743  reward mean:- 48.9375  reward bound:- 47.0\n",
            "loss:- 0.675475537776947  reward mean:- 33.875  reward bound:- 34.5\n",
            "loss:- 0.6697324514389038  reward mean:- 32.5625  reward bound:- 34.5\n",
            "loss:- 0.6909660696983337  reward mean:- 29.625  reward bound:- 39.5\n",
            "loss:- 0.6606252193450928  reward mean:- 45.125  reward bound:- 52.5\n",
            "loss:- 0.673507034778595  reward mean:- 31.125  reward bound:- 38.0\n",
            "loss:- 0.6577177047729492  reward mean:- 36.1875  reward bound:- 43.0\n",
            "loss:- 0.6532575488090515  reward mean:- 39.1875  reward bound:- 43.5\n",
            "loss:- 0.6589432954788208  reward mean:- 30.3125  reward bound:- 37.0\n",
            "loss:- 0.6579434275627136  reward mean:- 43.1875  reward bound:- 49.5\n",
            "loss:- 0.6597633361816406  reward mean:- 36.6875  reward bound:- 43.0\n",
            "loss:- 0.6467599272727966  reward mean:- 36.625  reward bound:- 35.5\n",
            "loss:- 0.662395715713501  reward mean:- 29.25  reward bound:- 28.5\n",
            "loss:- 0.6491113305091858  reward mean:- 46.875  reward bound:- 55.0\n",
            "loss:- 0.6508105993270874  reward mean:- 46.875  reward bound:- 50.5\n",
            "loss:- 0.6593287587165833  reward mean:- 44.9375  reward bound:- 52.5\n",
            "loss:- 0.6621827483177185  reward mean:- 29.75  reward bound:- 36.5\n",
            "loss:- 0.6455780267715454  reward mean:- 35.75  reward bound:- 37.5\n",
            "loss:- 0.645921528339386  reward mean:- 39.5625  reward bound:- 47.5\n",
            "loss:- 0.6472878456115723  reward mean:- 52.25  reward bound:- 57.5\n",
            "loss:- 0.644957959651947  reward mean:- 32.8125  reward bound:- 33.5\n",
            "loss:- 0.6465144753456116  reward mean:- 34.6875  reward bound:- 34.5\n",
            "loss:- 0.6473721265792847  reward mean:- 37.9375  reward bound:- 41.0\n",
            "loss:- 0.6533154845237732  reward mean:- 43.8125  reward bound:- 50.0\n",
            "loss:- 0.6312732100486755  reward mean:- 29.4375  reward bound:- 34.5\n",
            "loss:- 0.6572405695915222  reward mean:- 38.125  reward bound:- 40.5\n",
            "loss:- 0.6453127264976501  reward mean:- 37.25  reward bound:- 47.5\n",
            "loss:- 0.642845094203949  reward mean:- 46.8125  reward bound:- 46.0\n",
            "loss:- 0.6444716453552246  reward mean:- 52.125  reward bound:- 56.0\n",
            "loss:- 0.6417506337165833  reward mean:- 58.625  reward bound:- 71.0\n",
            "loss:- 0.6426336169242859  reward mean:- 52.875  reward bound:- 61.0\n",
            "loss:- 0.6418963074684143  reward mean:- 54.9375  reward bound:- 65.0\n",
            "loss:- 0.6399050951004028  reward mean:- 47.125  reward bound:- 46.5\n",
            "loss:- 0.6370481252670288  reward mean:- 47.375  reward bound:- 54.0\n",
            "loss:- 0.6374662518501282  reward mean:- 46.25  reward bound:- 65.0\n",
            "loss:- 0.6331976056098938  reward mean:- 51.9375  reward bound:- 61.5\n",
            "loss:- 0.6314449906349182  reward mean:- 51.0625  reward bound:- 66.5\n",
            "loss:- 0.6306091547012329  reward mean:- 36.9375  reward bound:- 46.5\n",
            "loss:- 0.6275820732116699  reward mean:- 44.0625  reward bound:- 49.0\n",
            "loss:- 0.6275582313537598  reward mean:- 44.625  reward bound:- 41.0\n",
            "loss:- 0.6480022668838501  reward mean:- 47.0625  reward bound:- 53.0\n",
            "loss:- 0.631943941116333  reward mean:- 62.4375  reward bound:- 67.0\n",
            "loss:- 0.634269118309021  reward mean:- 38.25  reward bound:- 47.0\n",
            "loss:- 0.6309807300567627  reward mean:- 62.375  reward bound:- 80.0\n",
            "loss:- 0.6393296122550964  reward mean:- 57.625  reward bound:- 67.5\n",
            "loss:- 0.6255388855934143  reward mean:- 47.4375  reward bound:- 54.5\n",
            "loss:- 0.6394773721694946  reward mean:- 38.9375  reward bound:- 41.5\n",
            "loss:- 0.6197580099105835  reward mean:- 49.8125  reward bound:- 58.5\n",
            "loss:- 0.6398903727531433  reward mean:- 61.3125  reward bound:- 71.5\n",
            "loss:- 0.6292458772659302  reward mean:- 63.875  reward bound:- 74.5\n",
            "loss:- 0.6354943513870239  reward mean:- 62.4375  reward bound:- 75.5\n",
            "loss:- 0.6307743191719055  reward mean:- 49.1875  reward bound:- 52.0\n",
            "loss:- 0.6261714696884155  reward mean:- 65.8125  reward bound:- 81.5\n",
            "loss:- 0.629581093788147  reward mean:- 69.5  reward bound:- 74.0\n",
            "loss:- 0.628197968006134  reward mean:- 58.125  reward bound:- 65.5\n",
            "loss:- 0.6211481094360352  reward mean:- 53.125  reward bound:- 49.5\n",
            "loss:- 0.6235367655754089  reward mean:- 54.75  reward bound:- 70.5\n",
            "loss:- 0.6221783757209778  reward mean:- 65.1875  reward bound:- 85.5\n",
            "loss:- 0.6119862794876099  reward mean:- 66.3125  reward bound:- 91.0\n",
            "loss:- 0.6254940032958984  reward mean:- 61.875  reward bound:- 82.0\n",
            "loss:- 0.6282658576965332  reward mean:- 40.8125  reward bound:- 43.5\n",
            "loss:- 0.6294898986816406  reward mean:- 66.5625  reward bound:- 81.0\n",
            "loss:- 0.6262089610099792  reward mean:- 50.25  reward bound:- 58.5\n",
            "loss:- 0.6189101934432983  reward mean:- 59.5  reward bound:- 66.0\n",
            "loss:- 0.624255359172821  reward mean:- 71.0  reward bound:- 87.0\n",
            "loss:- 0.6108471751213074  reward mean:- 51.0  reward bound:- 63.0\n",
            "loss:- 0.6270013451576233  reward mean:- 69.1875  reward bound:- 91.0\n",
            "loss:- 0.6234099268913269  reward mean:- 67.9375  reward bound:- 85.5\n",
            "loss:- 0.6143054962158203  reward mean:- 71.75  reward bound:- 88.0\n",
            "loss:- 0.6253746151924133  reward mean:- 58.625  reward bound:- 73.0\n",
            "loss:- 0.6141476631164551  reward mean:- 56.5625  reward bound:- 70.5\n",
            "loss:- 0.621723473072052  reward mean:- 59.125  reward bound:- 74.5\n",
            "loss:- 0.6068797707557678  reward mean:- 79.875  reward bound:- 96.0\n",
            "loss:- 0.6198762655258179  reward mean:- 80.5  reward bound:- 95.5\n",
            "loss:- 0.6242066025733948  reward mean:- 44.75  reward bound:- 50.0\n",
            "loss:- 0.6153696179389954  reward mean:- 85.5625  reward bound:- 120.0\n",
            "loss:- 0.6098686456680298  reward mean:- 82.9375  reward bound:- 115.5\n",
            "loss:- 0.6173551678657532  reward mean:- 64.8125  reward bound:- 76.0\n",
            "loss:- 0.6178455352783203  reward mean:- 88.4375  reward bound:- 102.0\n",
            "loss:- 0.6130595803260803  reward mean:- 76.0  reward bound:- 87.5\n",
            "loss:- 0.6194972991943359  reward mean:- 87.0  reward bound:- 83.5\n",
            "loss:- 0.6184453964233398  reward mean:- 78.3125  reward bound:- 104.5\n",
            "loss:- 0.6149229407310486  reward mean:- 76.875  reward bound:- 96.0\n",
            "loss:- 0.624670147895813  reward mean:- 59.875  reward bound:- 63.0\n",
            "loss:- 0.6035653352737427  reward mean:- 84.0625  reward bound:- 108.0\n",
            "loss:- 0.6040395498275757  reward mean:- 71.0  reward bound:- 76.0\n",
            "loss:- 0.6036080121994019  reward mean:- 96.25  reward bound:- 116.0\n",
            "loss:- 0.6196268796920776  reward mean:- 95.3125  reward bound:- 122.0\n",
            "loss:- 0.6134997010231018  reward mean:- 97.6875  reward bound:- 132.5\n",
            "loss:- 0.622654914855957  reward mean:- 82.6875  reward bound:- 119.0\n",
            "loss:- 0.6096382737159729  reward mean:- 76.375  reward bound:- 111.5\n",
            "loss:- 0.606173038482666  reward mean:- 85.5  reward bound:- 85.5\n",
            "loss:- 0.6157910823822021  reward mean:- 80.75  reward bound:- 93.5\n",
            "loss:- 0.608473539352417  reward mean:- 79.1875  reward bound:- 106.5\n",
            "loss:- 0.6018547415733337  reward mean:- 89.5625  reward bound:- 122.0\n",
            "loss:- 0.6086974740028381  reward mean:- 80.5  reward bound:- 117.5\n",
            "loss:- 0.6074883937835693  reward mean:- 92.375  reward bound:- 113.0\n",
            "loss:- 0.6150877475738525  reward mean:- 84.1875  reward bound:- 99.5\n",
            "loss:- 0.6043292284011841  reward mean:- 117.6875  reward bound:- 160.5\n",
            "loss:- 0.6068844795227051  reward mean:- 113.0625  reward bound:- 181.5\n",
            "loss:- 0.6238250732421875  reward mean:- 114.75  reward bound:- 132.5\n",
            "loss:- 0.615937352180481  reward mean:- 102.25  reward bound:- 143.5\n",
            "loss:- 0.5988735556602478  reward mean:- 85.75  reward bound:- 98.0\n",
            "loss:- 0.6169445514678955  reward mean:- 100.75  reward bound:- 120.0\n",
            "loss:- 0.6116675138473511  reward mean:- 105.3125  reward bound:- 135.0\n",
            "loss:- 0.6051202416419983  reward mean:- 90.5  reward bound:- 111.0\n",
            "loss:- 0.6041207909584045  reward mean:- 137.25  reward bound:- 185.0\n",
            "loss:- 0.6160086393356323  reward mean:- 99.1875  reward bound:- 121.0\n",
            "loss:- 0.611757755279541  reward mean:- 84.1875  reward bound:- 118.0\n",
            "loss:- 0.6078141331672668  reward mean:- 109.375  reward bound:- 124.0\n",
            "loss:- 0.6140607595443726  reward mean:- 92.3125  reward bound:- 113.5\n",
            "loss:- 0.6010963320732117  reward mean:- 99.75  reward bound:- 132.0\n",
            "loss:- 0.6069813966751099  reward mean:- 98.125  reward bound:- 136.0\n",
            "loss:- 0.6087281703948975  reward mean:- 127.9375  reward bound:- 161.0\n",
            "loss:- 0.6064010858535767  reward mean:- 123.6875  reward bound:- 160.5\n",
            "loss:- 0.6018643975257874  reward mean:- 127.875  reward bound:- 170.0\n",
            "loss:- 0.600592315196991  reward mean:- 131.9375  reward bound:- 157.5\n",
            "loss:- 0.5946145057678223  reward mean:- 119.6875  reward bound:- 176.0\n",
            "loss:- 0.601557731628418  reward mean:- 92.1875  reward bound:- 120.0\n",
            "loss:- 0.6011010408401489  reward mean:- 140.75  reward bound:- 164.5\n",
            "loss:- 0.610640823841095  reward mean:- 112.625  reward bound:- 135.5\n",
            "loss:- 0.6112168431282043  reward mean:- 121.5625  reward bound:- 151.0\n",
            "loss:- 0.6068835258483887  reward mean:- 103.5  reward bound:- 133.5\n",
            "loss:- 0.6096880435943604  reward mean:- 139.1875  reward bound:- 171.5\n",
            "loss:- 0.600068986415863  reward mean:- 128.4375  reward bound:- 194.5\n",
            "loss:- 0.6004675030708313  reward mean:- 126.3125  reward bound:- 174.5\n",
            "loss:- 0.5963851809501648  reward mean:- 117.75  reward bound:- 146.0\n",
            "loss:- 0.598343551158905  reward mean:- 119.4375  reward bound:- 144.0\n",
            "loss:- 0.5968314409255981  reward mean:- 134.75  reward bound:- 187.0\n",
            "loss:- 0.593522846698761  reward mean:- 117.0  reward bound:- 157.0\n",
            "loss:- 0.5938764810562134  reward mean:- 138.0  reward bound:- 179.5\n",
            "loss:- 0.6011053323745728  reward mean:- 146.1875  reward bound:- 182.0\n",
            "loss:- 0.6007606387138367  reward mean:- 139.625  reward bound:- 174.5\n",
            "loss:- 0.6004680395126343  reward mean:- 130.375  reward bound:- 171.5\n",
            "loss:- 0.6044928431510925  reward mean:- 124.9375  reward bound:- 187.0\n",
            "loss:- 0.5964707136154175  reward mean:- 157.0625  reward bound:- 173.5\n",
            "loss:- 0.6080061197280884  reward mean:- 152.0  reward bound:- 191.5\n",
            "loss:- 0.5865027904510498  reward mean:- 149.6875  reward bound:- 168.5\n",
            "loss:- 0.5890117287635803  reward mean:- 135.8125  reward bound:- 183.5\n",
            "loss:- 0.5939532518386841  reward mean:- 171.1875  reward bound:- 200.0\n",
            "loss:- 0.5961426496505737  reward mean:- 136.5  reward bound:- 171.5\n",
            "loss:- 0.6045202612876892  reward mean:- 112.1875  reward bound:- 118.0\n",
            "loss:- 0.6043925881385803  reward mean:- 132.75  reward bound:- 162.0\n",
            "loss:- 0.5865048170089722  reward mean:- 150.75  reward bound:- 200.0\n",
            "loss:- 0.5926293134689331  reward mean:- 133.0  reward bound:- 180.5\n",
            "loss:- 0.5888767838478088  reward mean:- 131.4375  reward bound:- 184.0\n",
            "loss:- 0.5865992903709412  reward mean:- 140.4375  reward bound:- 186.5\n",
            "loss:- 0.5938754677772522  reward mean:- 142.4375  reward bound:- 164.0\n",
            "loss:- 0.5942303538322449  reward mean:- 154.9375  reward bound:- 200.0\n",
            "loss:- 0.5971218347549438  reward mean:- 143.625  reward bound:- 200.0\n",
            "loss:- 0.5945889949798584  reward mean:- 159.0  reward bound:- 200.0\n",
            "loss:- 0.585352897644043  reward mean:- 157.125  reward bound:- 192.5\n",
            "loss:- 0.5983538627624512  reward mean:- 157.3125  reward bound:- 185.5\n",
            "loss:- 0.5857093334197998  reward mean:- 155.4375  reward bound:- 200.0\n",
            "loss:- 0.5940255522727966  reward mean:- 143.625  reward bound:- 178.0\n",
            "loss:- 0.5915103554725647  reward mean:- 152.8125  reward bound:- 188.0\n",
            "loss:- 0.5948404669761658  reward mean:- 150.5  reward bound:- 200.0\n",
            "loss:- 0.5905636548995972  reward mean:- 144.0  reward bound:- 176.0\n",
            "loss:- 0.5884822010993958  reward mean:- 143.0625  reward bound:- 200.0\n",
            "loss:- 0.5934348702430725  reward mean:- 172.125  reward bound:- 200.0\n",
            "loss:- 0.5821017026901245  reward mean:- 152.625  reward bound:- 192.5\n",
            "loss:- 0.5892744660377502  reward mean:- 165.75  reward bound:- 200.0\n",
            "loss:- 0.586105227470398  reward mean:- 165.125  reward bound:- 200.0\n",
            "loss:- 0.5773694515228271  reward mean:- 154.6875  reward bound:- 200.0\n",
            "loss:- 0.5954781770706177  reward mean:- 164.5  reward bound:- 200.0\n",
            "loss:- 0.5778594017028809  reward mean:- 152.625  reward bound:- 194.5\n",
            "loss:- 0.5945613384246826  reward mean:- 153.4375  reward bound:- 200.0\n",
            "loss:- 0.5841493010520935  reward mean:- 151.4375  reward bound:- 200.0\n",
            "loss:- 0.5925589203834534  reward mean:- 170.8125  reward bound:- 200.0\n",
            "loss:- 0.5879136919975281  reward mean:- 160.625  reward bound:- 200.0\n",
            "loss:- 0.5858206152915955  reward mean:- 176.8125  reward bound:- 200.0\n",
            "loss:- 0.5810487270355225  reward mean:- 159.875  reward bound:- 200.0\n",
            "loss:- 0.5884648561477661  reward mean:- 194.1875  reward bound:- 200.0\n",
            "loss:- 0.5774471759796143  reward mean:- 165.5625  reward bound:- 200.0\n",
            "loss:- 0.5750056505203247  reward mean:- 177.6875  reward bound:- 200.0\n",
            "loss:- 0.5919296145439148  reward mean:- 168.875  reward bound:- 200.0\n",
            "loss:- 0.5828425884246826  reward mean:- 186.125  reward bound:- 200.0\n",
            "loss:- 0.5861048698425293  reward mean:- 172.875  reward bound:- 200.0\n",
            "loss:- 0.5840844511985779  reward mean:- 174.3125  reward bound:- 200.0\n",
            "loss:- 0.5863820314407349  reward mean:- 181.0625  reward bound:- 200.0\n",
            "loss:- 0.5938007831573486  reward mean:- 175.6875  reward bound:- 200.0\n",
            "loss:- 0.592362642288208  reward mean:- 177.9375  reward bound:- 200.0\n",
            "loss:- 0.5786643624305725  reward mean:- 170.3125  reward bound:- 200.0\n",
            "loss:- 0.5817826986312866  reward mean:- 181.25  reward bound:- 200.0\n",
            "loss:- 0.5778398513793945  reward mean:- 171.9375  reward bound:- 200.0\n",
            "loss:- 0.5802198052406311  reward mean:- 167.9375  reward bound:- 200.0\n",
            "loss:- 0.5752953290939331  reward mean:- 179.125  reward bound:- 200.0\n",
            "loss:- 0.5853068232536316  reward mean:- 177.6875  reward bound:- 200.0\n",
            "loss:- 0.5784626603126526  reward mean:- 180.3125  reward bound:- 200.0\n",
            "loss:- 0.5824125409126282  reward mean:- 185.875  reward bound:- 200.0\n",
            "loss:- 0.5861161947250366  reward mean:- 158.625  reward bound:- 200.0\n",
            "loss:- 0.5815988779067993  reward mean:- 171.3125  reward bound:- 200.0\n",
            "loss:- 0.5715224742889404  reward mean:- 185.0  reward bound:- 200.0\n",
            "loss:- 0.5715419054031372  reward mean:- 162.4375  reward bound:- 200.0\n",
            "loss:- 0.5795965790748596  reward mean:- 175.375  reward bound:- 200.0\n",
            "loss:- 0.576002299785614  reward mean:- 191.25  reward bound:- 200.0\n",
            "loss:- 0.5758769512176514  reward mean:- 171.5  reward bound:- 200.0\n",
            "loss:- 0.5732554793357849  reward mean:- 185.4375  reward bound:- 200.0\n",
            "loss:- 0.587446391582489  reward mean:- 182.75  reward bound:- 200.0\n",
            "loss:- 0.576154887676239  reward mean:- 198.9375  reward bound:- 200.0\n",
            "loss:- 0.5695494413375854  reward mean:- 186.5  reward bound:- 200.0\n",
            "loss:- 0.5792253613471985  reward mean:- 157.125  reward bound:- 200.0\n",
            "loss:- 0.5763623118400574  reward mean:- 179.6875  reward bound:- 200.0\n",
            "loss:- 0.5745121240615845  reward mean:- 177.625  reward bound:- 200.0\n",
            "loss:- 0.573354184627533  reward mean:- 167.0625  reward bound:- 200.0\n",
            "loss:- 0.5795360207557678  reward mean:- 173.5  reward bound:- 200.0\n",
            "loss:- 0.5822887420654297  reward mean:- 191.75  reward bound:- 200.0\n",
            "loss:- 0.5727037191390991  reward mean:- 184.6875  reward bound:- 200.0\n",
            "loss:- 0.5793222188949585  reward mean:- 160.1875  reward bound:- 200.0\n",
            "loss:- 0.5808460712432861  reward mean:- 186.6875  reward bound:- 200.0\n",
            "loss:- 0.5876863598823547  reward mean:- 189.1875  reward bound:- 200.0\n",
            "loss:- 0.5755801200866699  reward mean:- 197.875  reward bound:- 200.0\n",
            "loss:- 0.5676689147949219  reward mean:- 178.5  reward bound:- 200.0\n",
            "loss:- 0.5751230120658875  reward mean:- 191.125  reward bound:- 200.0\n",
            "loss:- 0.5761364102363586  reward mean:- 172.625  reward bound:- 200.0\n",
            "loss:- 0.5680821537971497  reward mean:- 180.1875  reward bound:- 200.0\n",
            "loss:- 0.5652509331703186  reward mean:- 185.5  reward bound:- 200.0\n",
            "loss:- 0.5723904967308044  reward mean:- 179.625  reward bound:- 200.0\n",
            "loss:- 0.5784303545951843  reward mean:- 182.3125  reward bound:- 200.0\n",
            "loss:- 0.5803686380386353  reward mean:- 185.75  reward bound:- 200.0\n",
            "loss:- 0.5675646662712097  reward mean:- 183.875  reward bound:- 200.0\n",
            "loss:- 0.5705940127372742  reward mean:- 189.4375  reward bound:- 200.0\n",
            "loss:- 0.5686870217323303  reward mean:- 176.25  reward bound:- 200.0\n",
            "loss:- 0.5738255977630615  reward mean:- 197.6875  reward bound:- 200.0\n",
            "loss:- 0.5763025879859924  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.576676070690155  reward mean:- 187.5625  reward bound:- 200.0\n",
            "loss:- 0.5713211894035339  reward mean:- 198.375  reward bound:- 200.0\n",
            "loss:- 0.5676386952400208  reward mean:- 186.875  reward bound:- 200.0\n",
            "loss:- 0.5721352696418762  reward mean:- 192.125  reward bound:- 200.0\n",
            "loss:- 0.5700179934501648  reward mean:- 177.4375  reward bound:- 200.0\n",
            "loss:- 0.5782513618469238  reward mean:- 195.3125  reward bound:- 200.0\n",
            "loss:- 0.5678247213363647  reward mean:- 183.75  reward bound:- 200.0\n",
            "loss:- 0.5741393566131592  reward mean:- 188.0  reward bound:- 200.0\n",
            "loss:- 0.5685612559318542  reward mean:- 194.3125  reward bound:- 200.0\n",
            "loss:- 0.5668718814849854  reward mean:- 186.0  reward bound:- 200.0\n",
            "loss:- 0.5801101326942444  reward mean:- 185.625  reward bound:- 200.0\n",
            "loss:- 0.5745198130607605  reward mean:- 180.3125  reward bound:- 200.0\n",
            "loss:- 0.5789065361022949  reward mean:- 191.125  reward bound:- 200.0\n",
            "loss:- 0.5676976442337036  reward mean:- 192.625  reward bound:- 200.0\n",
            "loss:- 0.5706269145011902  reward mean:- 173.25  reward bound:- 200.0\n",
            "loss:- 0.5620725750923157  reward mean:- 184.75  reward bound:- 200.0\n",
            "loss:- 0.5661777257919312  reward mean:- 190.625  reward bound:- 200.0\n",
            "loss:- 0.5575754642486572  reward mean:- 195.75  reward bound:- 200.0\n",
            "loss:- 0.5670472979545593  reward mean:- 187.25  reward bound:- 200.0\n",
            "loss:- 0.5712924599647522  reward mean:- 198.0  reward bound:- 200.0\n",
            "loss:- 0.5702628493309021  reward mean:- 189.75  reward bound:- 200.0\n",
            "loss:- 0.5649263858795166  reward mean:- 197.25  reward bound:- 200.0\n",
            "loss:- 0.5691751837730408  reward mean:- 186.625  reward bound:- 200.0\n",
            "loss:- 0.56373530626297  reward mean:- 192.3125  reward bound:- 200.0\n",
            "loss:- 0.5683668851852417  reward mean:- 191.0625  reward bound:- 200.0\n",
            "loss:- 0.5669937133789062  reward mean:- 182.375  reward bound:- 200.0\n",
            "loss:- 0.5618743300437927  reward mean:- 184.9375  reward bound:- 200.0\n",
            "loss:- 0.5612514019012451  reward mean:- 182.6875  reward bound:- 200.0\n",
            "loss:- 0.5640217065811157  reward mean:- 198.75  reward bound:- 200.0\n",
            "loss:- 0.5730077028274536  reward mean:- 198.5  reward bound:- 200.0\n",
            "loss:- 0.5661003589630127  reward mean:- 188.9375  reward bound:- 200.0\n",
            "loss:- 0.5602962970733643  reward mean:- 194.125  reward bound:- 200.0\n",
            "loss:- 0.5637465715408325  reward mean:- 196.8125  reward bound:- 200.0\n",
            "loss:- 0.5617362260818481  reward mean:- 188.375  reward bound:- 200.0\n",
            "loss:- 0.5647774338722229  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5652600526809692  reward mean:- 197.5625  reward bound:- 200.0\n",
            "loss:- 0.5687368512153625  reward mean:- 194.4375  reward bound:- 200.0\n",
            "loss:- 0.5599711537361145  reward mean:- 199.5  reward bound:- 200.0\n",
            "loss:- 0.5632627606391907  reward mean:- 193.625  reward bound:- 200.0\n",
            "loss:- 0.5686628222465515  reward mean:- 186.0  reward bound:- 200.0\n",
            "loss:- 0.5575447082519531  reward mean:- 188.0625  reward bound:- 200.0\n",
            "loss:- 0.5655836462974548  reward mean:- 189.125  reward bound:- 200.0\n",
            "loss:- 0.55853670835495  reward mean:- 196.4375  reward bound:- 200.0\n",
            "loss:- 0.5668366551399231  reward mean:- 192.9375  reward bound:- 200.0\n",
            "loss:- 0.5631496906280518  reward mean:- 196.4375  reward bound:- 200.0\n",
            "loss:- 0.5673608183860779  reward mean:- 197.625  reward bound:- 200.0\n",
            "loss:- 0.5706984400749207  reward mean:- 189.8125  reward bound:- 200.0\n",
            "loss:- 0.5650280714035034  reward mean:- 197.625  reward bound:- 200.0\n",
            "loss:- 0.5638501644134521  reward mean:- 195.1875  reward bound:- 200.0\n",
            "loss:- 0.5589127540588379  reward mean:- 192.625  reward bound:- 200.0\n",
            "loss:- 0.5606869459152222  reward mean:- 187.4375  reward bound:- 200.0\n",
            "loss:- 0.566133439540863  reward mean:- 192.875  reward bound:- 200.0\n",
            "loss:- 0.5665236711502075  reward mean:- 194.6875  reward bound:- 200.0\n",
            "loss:- 0.5592976212501526  reward mean:- 199.375  reward bound:- 200.0\n",
            "loss:- 0.5613250136375427  reward mean:- 196.4375  reward bound:- 200.0\n",
            "loss:- 0.5614762306213379  reward mean:- 193.3125  reward bound:- 200.0\n",
            "loss:- 0.561176598072052  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5584889650344849  reward mean:- 192.0625  reward bound:- 200.0\n",
            "loss:- 0.5596968531608582  reward mean:- 192.625  reward bound:- 200.0\n",
            "loss:- 0.5565526485443115  reward mean:- 190.9375  reward bound:- 200.0\n",
            "loss:- 0.5535062551498413  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5569124817848206  reward mean:- 199.875  reward bound:- 200.0\n",
            "loss:- 0.5583629608154297  reward mean:- 186.375  reward bound:- 200.0\n",
            "loss:- 0.5599617958068848  reward mean:- 199.3125  reward bound:- 200.0\n",
            "loss:- 0.5666859745979309  reward mean:- 199.375  reward bound:- 200.0\n",
            "loss:- 0.5489112734794617  reward mean:- 186.5  reward bound:- 200.0\n",
            "loss:- 0.5678693652153015  reward mean:- 197.25  reward bound:- 200.0\n",
            "loss:- 0.562475860118866  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5521399974822998  reward mean:- 190.1875  reward bound:- 200.0\n",
            "loss:- 0.5628786087036133  reward mean:- 197.9375  reward bound:- 200.0\n",
            "loss:- 0.5600046515464783  reward mean:- 183.5  reward bound:- 200.0\n",
            "loss:- 0.5565522909164429  reward mean:- 192.5  reward bound:- 200.0\n",
            "loss:- 0.5707920789718628  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5591893792152405  reward mean:- 189.6875  reward bound:- 200.0\n",
            "loss:- 0.5567410588264465  reward mean:- 192.875  reward bound:- 200.0\n",
            "loss:- 0.5623658895492554  reward mean:- 191.875  reward bound:- 200.0\n",
            "loss:- 0.5554518103599548  reward mean:- 191.9375  reward bound:- 200.0\n",
            "loss:- 0.5582764744758606  reward mean:- 193.625  reward bound:- 200.0\n",
            "loss:- 0.5586069822311401  reward mean:- 189.375  reward bound:- 200.0\n",
            "loss:- 0.5642828345298767  reward mean:- 189.0625  reward bound:- 200.0\n",
            "loss:- 0.5647792816162109  reward mean:- 199.8125  reward bound:- 200.0\n",
            "loss:- 0.5529291033744812  reward mean:- 196.25  reward bound:- 200.0\n",
            "loss:- 0.5623067617416382  reward mean:- 189.0  reward bound:- 200.0\n",
            "loss:- 0.554956316947937  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.566388726234436  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5536292195320129  reward mean:- 195.375  reward bound:- 200.0\n",
            "loss:- 0.5596508979797363  reward mean:- 193.9375  reward bound:- 200.0\n",
            "loss:- 0.5514994263648987  reward mean:- 184.125  reward bound:- 200.0\n",
            "loss:- 0.5530859231948853  reward mean:- 191.25  reward bound:- 200.0\n",
            "loss:- 0.5525806546211243  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5712451338768005  reward mean:- 195.875  reward bound:- 200.0\n",
            "loss:- 0.559836745262146  reward mean:- 191.375  reward bound:- 200.0\n",
            "loss:- 0.5544804334640503  reward mean:- 198.6875  reward bound:- 200.0\n",
            "loss:- 0.5546717643737793  reward mean:- 195.0625  reward bound:- 200.0\n",
            "loss:- 0.5553833246231079  reward mean:- 197.0  reward bound:- 200.0\n",
            "loss:- 0.5605039000511169  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.558213472366333  reward mean:- 186.375  reward bound:- 200.0\n",
            "loss:- 0.5542278289794922  reward mean:- 190.5  reward bound:- 200.0\n",
            "loss:- 0.5602117776870728  reward mean:- 192.1875  reward bound:- 200.0\n",
            "loss:- 0.5543895363807678  reward mean:- 196.625  reward bound:- 200.0\n",
            "loss:- 0.5629685521125793  reward mean:- 197.75  reward bound:- 200.0\n",
            "loss:- 0.5672066807746887  reward mean:- 190.5625  reward bound:- 200.0\n",
            "loss:- 0.5561351776123047  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5496339797973633  reward mean:- 193.6875  reward bound:- 200.0\n",
            "loss:- 0.559836745262146  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5623709559440613  reward mean:- 197.1875  reward bound:- 200.0\n",
            "loss:- 0.5601444244384766  reward mean:- 198.5  reward bound:- 200.0\n",
            "loss:- 0.5488379597663879  reward mean:- 197.75  reward bound:- 200.0\n",
            "loss:- 0.5570993423461914  reward mean:- 197.625  reward bound:- 200.0\n",
            "loss:- 0.5593506097793579  reward mean:- 196.4375  reward bound:- 200.0\n",
            "loss:- 0.5578510165214539  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5635250210762024  reward mean:- 190.125  reward bound:- 200.0\n",
            "loss:- 0.5498804450035095  reward mean:- 192.1875  reward bound:- 200.0\n",
            "loss:- 0.5495490431785583  reward mean:- 194.125  reward bound:- 200.0\n",
            "loss:- 0.5598633289337158  reward mean:- 196.625  reward bound:- 200.0\n",
            "loss:- 0.5528187155723572  reward mean:- 192.25  reward bound:- 200.0\n",
            "loss:- 0.5548056960105896  reward mean:- 196.8125  reward bound:- 200.0\n",
            "loss:- 0.5593996644020081  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.554404616355896  reward mean:- 198.9375  reward bound:- 200.0\n",
            "loss:- 0.5516730546951294  reward mean:- 199.5  reward bound:- 200.0\n",
            "loss:- 0.5687020421028137  reward mean:- 188.25  reward bound:- 200.0\n",
            "loss:- 0.5625450611114502  reward mean:- 197.3125  reward bound:- 200.0\n",
            "loss:- 0.5496925711631775  reward mean:- 197.0  reward bound:- 200.0\n",
            "loss:- 0.5474649667739868  reward mean:- 196.75  reward bound:- 200.0\n",
            "loss:- 0.5535465478897095  reward mean:- 199.1875  reward bound:- 200.0\n",
            "loss:- 0.5519136786460876  reward mean:- 191.9375  reward bound:- 200.0\n",
            "loss:- 0.5505257248878479  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5618384480476379  reward mean:- 187.9375  reward bound:- 200.0\n",
            "loss:- 0.5635907649993896  reward mean:- 195.625  reward bound:- 200.0\n",
            "loss:- 0.5582485198974609  reward mean:- 194.25  reward bound:- 200.0\n",
            "loss:- 0.5536637306213379  reward mean:- 196.0  reward bound:- 200.0\n",
            "loss:- 0.5491288900375366  reward mean:- 199.625  reward bound:- 200.0\n",
            "loss:- 0.5576819777488708  reward mean:- 195.3125  reward bound:- 200.0\n",
            "loss:- 0.5565198659896851  reward mean:- 195.4375  reward bound:- 200.0\n",
            "loss:- 0.553508460521698  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5519856810569763  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5493055582046509  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5578977465629578  reward mean:- 198.1875  reward bound:- 200.0\n",
            "loss:- 0.5539160370826721  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5575166344642639  reward mean:- 194.8125  reward bound:- 200.0\n",
            "loss:- 0.5519457459449768  reward mean:- 192.25  reward bound:- 200.0\n",
            "loss:- 0.5376924872398376  reward mean:- 189.8125  reward bound:- 200.0\n",
            "loss:- 0.5566025972366333  reward mean:- 197.125  reward bound:- 200.0\n",
            "loss:- 0.55845046043396  reward mean:- 198.6875  reward bound:- 200.0\n",
            "loss:- 0.5591498613357544  reward mean:- 198.8125  reward bound:- 200.0\n",
            "loss:- 0.5573974251747131  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5529109835624695  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5556171536445618  reward mean:- 196.5  reward bound:- 200.0\n",
            "loss:- 0.5566001534461975  reward mean:- 197.0625  reward bound:- 200.0\n",
            "loss:- 0.5562055110931396  reward mean:- 191.0625  reward bound:- 200.0\n",
            "loss:- 0.5544638633728027  reward mean:- 197.3125  reward bound:- 200.0\n",
            "loss:- 0.5507487654685974  reward mean:- 192.9375  reward bound:- 200.0\n",
            "loss:- 0.5519307851791382  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5552176833152771  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5547680854797363  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5532004237174988  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5560675859451294  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5550990700721741  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5533189177513123  reward mean:- 196.125  reward bound:- 200.0\n",
            "loss:- 0.5510734915733337  reward mean:- 188.6875  reward bound:- 200.0\n",
            "loss:- 0.5522514581680298  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.550104022026062  reward mean:- 190.75  reward bound:- 200.0\n",
            "loss:- 0.5530545711517334  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5472403764724731  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5510125160217285  reward mean:- 196.6875  reward bound:- 200.0\n",
            "loss:- 0.5522135496139526  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5547089576721191  reward mean:- 198.0625  reward bound:- 200.0\n",
            "loss:- 0.5472991466522217  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5496231317520142  reward mean:- 196.875  reward bound:- 200.0\n",
            "loss:- 0.5459080338478088  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5610501766204834  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5500165224075317  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5512826442718506  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5569874048233032  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5616707801818848  reward mean:- 192.8125  reward bound:- 200.0\n",
            "loss:- 0.5556894540786743  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5566043257713318  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5549230575561523  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5583462715148926  reward mean:- 198.5  reward bound:- 200.0\n",
            "loss:- 0.5566312074661255  reward mean:- 195.6875  reward bound:- 200.0\n",
            "loss:- 0.5564584136009216  reward mean:- 197.6875  reward bound:- 200.0\n",
            "loss:- 0.5469561815261841  reward mean:- 197.125  reward bound:- 200.0\n",
            "loss:- 0.5491350293159485  reward mean:- 195.9375  reward bound:- 200.0\n",
            "loss:- 0.5494141578674316  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5539699792861938  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.555574893951416  reward mean:- 197.3125  reward bound:- 200.0\n",
            "loss:- 0.5521837472915649  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5447925329208374  reward mean:- 196.625  reward bound:- 200.0\n",
            "loss:- 0.5506670475006104  reward mean:- 198.625  reward bound:- 200.0\n",
            "loss:- 0.5486949682235718  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.559622049331665  reward mean:- 198.6875  reward bound:- 200.0\n",
            "loss:- 0.5530144572257996  reward mean:- 192.875  reward bound:- 200.0\n",
            "loss:- 0.5458641052246094  reward mean:- 185.375  reward bound:- 200.0\n",
            "loss:- 0.5538559556007385  reward mean:- 199.4375  reward bound:- 200.0\n",
            "loss:- 0.5537641048431396  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5564911365509033  reward mean:- 194.5625  reward bound:- 200.0\n",
            "loss:- 0.553943395614624  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5473119020462036  reward mean:- 189.4375  reward bound:- 200.0\n",
            "loss:- 0.5432485342025757  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5583270192146301  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5558326244354248  reward mean:- 198.0  reward bound:- 200.0\n",
            "loss:- 0.555415153503418  reward mean:- 193.625  reward bound:- 200.0\n",
            "loss:- 0.5519078373908997  reward mean:- 197.5  reward bound:- 200.0\n",
            "loss:- 0.557304859161377  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5514407157897949  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5458600521087646  reward mean:- 197.375  reward bound:- 200.0\n",
            "loss:- 0.5513913631439209  reward mean:- 196.625  reward bound:- 200.0\n",
            "loss:- 0.5480774641036987  reward mean:- 197.1875  reward bound:- 200.0\n",
            "loss:- 0.5471815466880798  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5472397208213806  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5470466017723083  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5607739686965942  reward mean:- 194.375  reward bound:- 200.0\n",
            "loss:- 0.5497314929962158  reward mean:- 198.8125  reward bound:- 200.0\n",
            "loss:- 0.5488196015357971  reward mean:- 196.5625  reward bound:- 200.0\n",
            "loss:- 0.5461820960044861  reward mean:- 192.125  reward bound:- 200.0\n",
            "loss:- 0.5460155606269836  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5547055602073669  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5516435503959656  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5499748587608337  reward mean:- 194.9375  reward bound:- 200.0\n",
            "loss:- 0.5565809011459351  reward mean:- 199.5  reward bound:- 200.0\n",
            "loss:- 0.5508844256401062  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5451893210411072  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5487972497940063  reward mean:- 197.625  reward bound:- 200.0\n",
            "loss:- 0.5541772246360779  reward mean:- 195.375  reward bound:- 200.0\n",
            "loss:- 0.5520249009132385  reward mean:- 199.4375  reward bound:- 200.0\n",
            "loss:- 0.5545756220817566  reward mean:- 198.5  reward bound:- 200.0\n",
            "loss:- 0.5531493425369263  reward mean:- 192.625  reward bound:- 200.0\n",
            "loss:- 0.5502663254737854  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5456858277320862  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5458704829216003  reward mean:- 196.0625  reward bound:- 200.0\n",
            "loss:- 0.5514986515045166  reward mean:- 195.3125  reward bound:- 200.0\n",
            "loss:- 0.5495688319206238  reward mean:- 197.5625  reward bound:- 200.0\n",
            "loss:- 0.5537488460540771  reward mean:- 198.1875  reward bound:- 200.0\n",
            "loss:- 0.5491085648536682  reward mean:- 191.875  reward bound:- 200.0\n",
            "loss:- 0.5453668236732483  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5460236668586731  reward mean:- 198.875  reward bound:- 200.0\n",
            "loss:- 0.5538205504417419  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5455852150917053  reward mean:- 197.0  reward bound:- 200.0\n",
            "loss:- 0.5527998208999634  reward mean:- 199.125  reward bound:- 200.0\n",
            "loss:- 0.5461856126785278  reward mean:- 198.0  reward bound:- 200.0\n",
            "loss:- 0.5553048253059387  reward mean:- 198.0  reward bound:- 200.0\n",
            "loss:- 0.5495544672012329  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5522540807723999  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5443201661109924  reward mean:- 190.3125  reward bound:- 200.0\n",
            "loss:- 0.5469463467597961  reward mean:- 192.5  reward bound:- 200.0\n",
            "loss:- 0.5441630482673645  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.548719584941864  reward mean:- 196.3125  reward bound:- 200.0\n",
            "loss:- 0.5564725995063782  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5469201803207397  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5483543872833252  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5557156205177307  reward mean:- 197.375  reward bound:- 200.0\n",
            "loss:- 0.5396753549575806  reward mean:- 193.75  reward bound:- 200.0\n",
            "loss:- 0.5548948049545288  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5453838109970093  reward mean:- 200.0  reward bound:- 200.0\n",
            "loss:- 0.5476272106170654  reward mean:- 196.9375  reward bound:- 200.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiJfgeoc7fly",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "462aba89-faf4-4fe2-8c61-02377d3af22e"
      },
      "source": [
        "plt.imshow(env.render('rgb_array'))"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f80676287f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbFYh88CXsjh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "outputId": "151889aa-c8a6-4e5a-d2f2-4e8e957fcac4"
      },
      "source": [
        "import time\n",
        "obs=torch.FloatTensor(env.reset())\n",
        "sm=n.Softmax(dim=1)\n",
        "for i in range(10):\n",
        "  actionscalar=(net1.forward(obs)).data.numpy()\n",
        "  action=int(np.argmax(actionscalar))\n",
        "  #print(action)\n",
        "  #dnfsjnf\n",
        "  obs,reward,isdone,_=env.step(action)\n",
        "  obs=torch.FloatTensor(obs)\n",
        "  print(reward)\n",
        "  print(isdone)\n",
        "  if isdone:\n",
        "    obs=torch.FloatTensor(env.reset())\n",
        "  plt.imshow(env.render(\"rgb_array\"))\n",
        "  time.sleep(1)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n",
            "False\n",
            "1.0\n",
            "False\n",
            "1.0\n",
            "False\n",
            "1.0\n",
            "False\n",
            "1.0\n",
            "False\n",
            "1.0\n",
            "False\n",
            "1.0\n",
            "False\n",
            "1.0\n",
            "False\n",
            "1.0\n",
            "False\n",
            "1.0\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZwxuSerM/wOz0eokBxGfRAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utljE63JXxNY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "af50143d-cfac-496f-faf7-3d6ed841e9d3"
      },
      "source": [
        "sm=n.Softmax()\n",
        "obs=env.reset()\n",
        "print(net1.forward(torch.FloatTensor(obs)).data.numpy())\n"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.13524394 0.00174135]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HII4n66Dbl6d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5967ac55-25fa-4644-f859-ba1f241e5f29"
      },
      "source": [
        "actprob."
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Softmax(dim=tensor([ 0.1656, -0.0202], grad_fn=<AddBackward0>))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "St8ybriAbvFe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}