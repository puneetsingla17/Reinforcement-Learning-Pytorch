{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ActorCritic-PONGgame.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjX15cxWAGIY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as n\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as op\n",
        "import torch.nn.functional as f\n",
        "import torch.nn.utils as nutils\n",
        "import gym"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRzbq0fiCIYK",
        "colab_type": "code",
        "outputId": "dc1e6c22-e96e-4fd0-85da-70b211e5dd6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        }
      },
      "source": [
        "!pip install ptan"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ptan\n",
            "  Downloading https://files.pythonhosted.org/packages/91/cb/57f6d86625f2b24c008b0524ca29559683aa75d00afa38b6b44d7fcad25b/ptan-0.6.tar.gz\n",
            "Collecting torch==1.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/05/50a05de5337f7a924bb8bd70c6936230642233e424d6a9747ef1cfbde353/torch-1.3.0-cp36-cp36m-manylinux1_x86_64.whl (773.1MB)\n",
            "\u001b[K     |████████████████████████████████| 773.1MB 22kB/s \n",
            "\u001b[?25hRequirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from ptan) (0.17.1)\n",
            "Requirement already satisfied: atari-py in /usr/local/lib/python3.6/dist-packages (from ptan) (0.2.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from ptan) (1.18.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from ptan) (4.1.2.30)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->ptan) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->ptan) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->ptan) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym->ptan) (1.12.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->ptan) (0.16.0)\n",
            "Building wheels for collected packages: ptan\n",
            "  Building wheel for ptan (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ptan: filename=ptan-0.6-cp36-none-any.whl size=23502 sha256=3b43308e465a88d9168c437d776cfcb7d22a88fccf9eb16b7797b8d6721f6897\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/4b/2f/9a45fd39b0a614a2716bc6128a7f1adb4647f323a2d90783f2\n",
            "Successfully built ptan\n",
            "\u001b[31mERROR: torchvision 0.5.0 has requirement torch==1.4.0, but you'll have torch 1.3.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch, ptan\n",
            "  Found existing installation: torch 1.4.0\n",
            "    Uninstalling torch-1.4.0:\n",
            "      Successfully uninstalled torch-1.4.0\n",
            "Successfully installed ptan-0.6 torch-1.3.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOSTrfEVCM9e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import ptan"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrDdm859CNwh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GAMMA = 0.99\n",
        "LEARNING_RATE = 0.001\n",
        "ENTROPY_BETA = 0.01\n",
        "BATCH_SIZE = 128\n",
        "NUM_ENVS = 50\n",
        "\n",
        "REWARD_STEPS = 4\n",
        "CLIP_GRAD = 0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SpuYTujDUMC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class net(n.Module):\n",
        "  def __init__(self,obsshape,naction):\n",
        "    super().__init__()\n",
        "    self.cnn1=n.Conv2d(obsshape[0],32,kernel_size=8,stride=4)\n",
        "    self.cnn2=n.Conv2d(32,64,kernel_size=8,stride=4)\n",
        "    self.cnn3=n.Conv2d(64,128,kernel_size=4,stride=2)\n",
        "    dim1=self.outshape(obsshape)\n",
        "    self.fc1=n.Linear(dim1,128)\n",
        "    self.fc2=n.Linear(128,naction)\n",
        "    self.fc3=n.Linear(dim1,128)\n",
        "    self.fc4=n.Linear(128,1)\n",
        "\n",
        "  def forward(self,x):\n",
        "    out=f.relu(self.cnn1(x))\n",
        "    out=f.relu(self.cnn2(out))\n",
        "    out=f.relu(self.cnn3(out))\n",
        "    out1=n.Flatten(start_dim=1,end_dim=-1)(out)\n",
        "    out2=f.relu(self.fc1(out1))\n",
        "    out2=self.fc2(out2)\n",
        "\n",
        "    out3=f.relu(self.fc3(out1))\n",
        "    out3=self.fc3(out3)\n",
        "    return out2,out3\n",
        "  \n",
        "  def outshape(self,obsshape):\n",
        "    z=torch.zeros(1,*obsshape)\n",
        "    out=self.cnn3(self.cnn2(self.cnn1(z)))\n",
        "    return out.shape[1]*out.shape[2]*out.shape[3]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dow9U_oWEw_h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unpackbatch(batch,net,skipsteps):\n",
        "  batchstates=[]\n",
        "  batchactions=[]\n",
        "  batchrewards=[]\n",
        "  batchlaststates=[]\n",
        "  notidx=[]\n",
        "  count=0\n",
        "  for e in batch:\n",
        "    batchstates.append(np.array(e.state,copy=False))\n",
        "    batchactions.append(e.action)\n",
        "    batchrewards.append(e.reward)\n",
        "    if e.last_state is not None:\n",
        "      notidx.append(count)\n",
        "      batchlaststates.append(np.array(e.last_state,copy=False)) # copy=False doesnt copy array into new memory location use same\n",
        "\n",
        "    count+=1\n",
        "  \n",
        "  statesv=torch.FloatTensor(batchstates)\n",
        "  actionsv=torch.LongTensor(batchactions)\n",
        "  rewardsv=np.array(batchrewards,copy=False)\n",
        "  laststate=torch.FloatTensor(batchlaststates)\n",
        "\n",
        "  if notidx:\n",
        "    batchlstv=net.forward(laststate)[1]\n",
        "    batchlstv=batchlstv.data.cpu().numpy()[:,0]\n",
        "    rewardsv[notidx]+=(gamma**skipsteps)*batchlstv\n",
        "  \n",
        "  rewardstv=torch.FloatTensor(rewardsv)\n",
        "\n",
        "  return statesv,actionsv,rewardstv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08fC4y9kTWfR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zUJ0KbnW_fJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class scaledframe(gym.ObservationWrapper):\n",
        "  def __init_(self,env):\n",
        "    super().__init__(env)\n",
        "    self.env=env\n",
        "  \n",
        "  def observation(self,x):\n",
        "    return np.array(x).astype(np.float32)/255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6N2HGZn0TbkZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "makeenv=lambda : scaledframe(ptan.common.wrappers.wrap_dqn(gym.make(\"PongNoFrameskip-v4\")))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqEnhtFSZkUy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gamma=0.99\n",
        "skipsteps=10\n",
        "batchsize=128\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4EXZGudWxFf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "numenvs=10\n",
        "model=net([4,84,84],6)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKEPy0RMWnzI",
        "colab_type": "code",
        "outputId": "1f97d21b-147a-491e-ada1-5238fd258963",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "envs=[makeenv() for _ in range(numenvs)]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YdZEzJvW1-k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "agent=ptan.agent.PolicyAgent(lambda x:model.forward(x)[0],apply_softmax=True,preprocessor=ptan.agent.float32_preprocessor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAW8e595YtBT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "expsource=ptan.experience.ExperienceSourceFirstLast(envs,agent,gamma=gamma,steps_count=skipsteps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LReOEqUEhfWl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt=op.Adam(model.parameters(),lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5g2dlXyPTn7A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batchsize=128\n",
        "entropybeta=0.01\n",
        "clipgrad=0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBPfKWFoZ7rK",
        "colab_type": "code",
        "outputId": "63003716-3ee3-4da1-db18-22e923b112b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "batch=[]\n",
        "for e in expsource:\n",
        "  batch.append(e)\n",
        "\n",
        "  if len(batch)<batchsize:\n",
        "    continue\n",
        "  \n",
        "  batchs,batcha,batchr=unpackbatch(batch,model,skipsteps)\n",
        "  batch.clear()\n",
        "\n",
        "  opt.zero_grad()\n",
        "  logits,vals=model.forward(batchs)\n",
        "  lossvaluesv=f.mse_loss(vals.squeeze(-1),batchr)\n",
        "\n",
        "  logprobv=f.log_softmax(logits,dim=1)\n",
        "  adv=batchr-vals     # Q=r+gamma*V   # very important batchr represent actionvalue , vals represent value of state\n",
        "  logprobaction_v=adv*logprobv[range(batchsize),batcha]\n",
        "  losspolicyv=logprobaction_v.mean()\n",
        "\n",
        "  prob=f.softmax(logits,dim=1)\n",
        "  entropyloss=-entropybeta*((prob*logprobv).sum(dim=1)).mean()\n",
        "\n",
        "  losspolicyv.backward(retain_graph=True)\n",
        "\n",
        "  totalloss=lossvaluesv+entropyloss\n",
        "  totalloss.backward()\n",
        "\n",
        "  nutils.clip_grad_norm_(model.parameters(),clipgrad)\n",
        "  opt.step()\n",
        "  print(totalloss.item())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 128])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.4140785336494446\n",
            "0.02603548765182495\n",
            "0.3954366147518158\n",
            "0.4624864459037781\n",
            "0.3866102397441864\n",
            "1.4562320709228516\n",
            "1.9308923482894897\n",
            "4.245213985443115\n",
            "8.219496726989746\n",
            "12.11728572845459\n",
            "20.38503646850586\n",
            "26.304241180419922\n",
            "33.56495666503906\n",
            "43.41205596923828\n",
            "48.45710754394531\n",
            "51.72475051879883\n",
            "51.77461624145508\n",
            "48.644840240478516\n",
            "45.16239929199219\n",
            "44.94396209716797\n",
            "53.738765716552734\n",
            "60.512046813964844\n",
            "73.09741973876953\n",
            "79.88333129882812\n",
            "71.29640197753906\n",
            "63.684146881103516\n",
            "47.45795822143555\n",
            "29.648761749267578\n",
            "21.83549690246582\n",
            "12.172743797302246\n",
            "5.974066257476807\n",
            "3.566744804382324\n",
            "1.1588586568832397\n",
            "0.587730884552002\n",
            "0.30282923579216003\n",
            "0.41609227657318115\n",
            "0.24525751173496246\n",
            "0.1498563587665558\n",
            "0.34076976776123047\n",
            "0.09283381700515747\n",
            "0.3043673038482666\n",
            "0.37143757939338684\n",
            "0.07975877821445465\n",
            "0.5815410017967224\n",
            "0.4675080180168152\n",
            "0.29553452134132385\n",
            "1.1308178901672363\n",
            "1.0875436067581177\n",
            "1.8001641035079956\n",
            "3.5593910217285156\n",
            "4.7494330406188965\n",
            "8.958335876464844\n",
            "13.179726600646973\n",
            "19.05362892150879\n",
            "38.84574508666992\n",
            "37.59566879272461\n",
            "47.85089874267578\n",
            "59.014774322509766\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-f04b90a3adf2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mbatchs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatcha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatchr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munpackbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mskipsteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-49-354a1d0ec500>\u001b[0m in \u001b[0;36munpackbatch\u001b[0;34m(batch, net, skipsteps)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mcount\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0mstatesv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m   \u001b[0mactionsv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0mrewardsv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchrewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-1piBJlPmFQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}